{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëã **–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é, –ú–∏—Ö–∞–∏–ª!**\n",
    "\n",
    "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ê—Ä—Å–µ–Ω –ê–±–¥—É–ª–∏–Ω, –∏ —è –±—É–¥—É —Ç–≤–æ–∏–º —Ä–µ–≤—å—é–≤–µ—Ä–æ–º –ø–æ –ø—Ä–æ–µ–∫—Ç—É. –ü—Ä–µ–¥–ª–∞–≥–∞—é –æ–±—â–∞—Ç—å—Å—è –Ω–∞ ¬´—Ç—ã¬ª, –µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—Ç–∏–≤ =) –ú–æ—è —Ü–µ–ª—å ‚Äî –Ω–µ –ø–æ–∏—Å–∫ –æ—à–∏–±–æ–∫ –≤ —Ç–≤–æ–µ–π —Ä–∞–±–æ—Ç–µ, –∞ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –æ–ø—ã—Ç–æ–º –∏ –ø–æ–º–æ—á—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å –ø—Ä–æ–µ–∫—Ç, —á—Ç–æ–±—ã –Ω–∞ —à–∞–≥ –ø—Ä–∏–±–ª–∏–∑–∏—Ç—å—Å—è –∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –ø–æ Data Science.\n",
    "\n",
    "–Ø –≤–∏–∂—É, —á—Ç–æ —Ç—ã –ø—Ä–æ–¥–µ–ª–∞–ª –±–æ–ª—å—à—É—é —Ä–∞–±–æ—Ç—É, –æ–Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –Ω–∞ —Ö–æ—Ä–æ—à–µ–º —É—Ä–æ–≤–Ω–µ üòä –í —Ü–µ–ª–æ–º —Ç—ã –ø–æ–ª—É—á–∏–ª –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –ø—Ä–æ–µ–∫—Ç—É, –Ω–æ –µ—Å—Ç—å –Ω–µ–º–Ω–æ–≥–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, —è –æ—Ç–º–µ—Ç–∏–ª –∏—Ö –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö! \n",
    "\n",
    "–ü—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–∞–±–æ—Ç —è –¥–µ–ª–∞—é —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ó–µ–ª–µ–Ω—ã–º —Ü–≤–µ—Ç–æ–º</b> –æ—Ç–º–µ—á–µ–Ω—ã —É–¥–∞—á–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –æ–ø–∏—Ä–∞—Ç—å—Å—è –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö.</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>‚ö†Ô∏è –ñ–µ–ª—Ç—ã–º —Ü–≤–µ—Ç–æ–º</b> —è –æ—Ç–º–µ—Ç–∏–ª –ø—É–Ω–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–æ-–¥—Ä—É–≥–æ–º—É. –û–¥–Ω–æ-–¥–≤–∞ —Ç–∞–∫–∏—Ö –∑–∞–º–µ—á–∞–Ω–∏—è –≤ –ø—Ä–æ–µ–∫—Ç–µ –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è, –Ω–æ –µ—Å–ª–∏ –∏—Ö –º–Ω–æ–≥–æ ‚Äî –ø—Ä–æ–µ–∫—Ç —Å–ª–µ–¥—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å. </div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>üö´ –ö—Ä–∞—Å–Ω—ã–º —Ü–≤–µ—Ç–æ–º</b> –æ—Ç–º–µ—á–µ–Ω—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–ø—Ä–∞–≤–∏—Ç—å, —á—Ç–æ–±—ã –ø—Ä–∏–Ω—è—Ç—å –ø—Ä–æ–µ–∫—Ç. </div>\n",
    "\n",
    "–ï—Å–ª–∏ –∫–∞–∫–∏–µ-—Ç–æ –º–æ–º–µ–Ω—Ç—ã –≤ –∑–∞–¥–∞–Ω–∏–∏ –¥–ª—è —Ç–µ–±—è –±—ã–ª–∏ –Ω–µ–ø–æ–Ω—è—Ç–Ω—ã –∏ —É —Ç–µ–±—è –µ—Å—Ç—å –∫–æ –º–Ω–µ –≤–æ–ø—Ä–æ—Å—ã ‚Äî —Å–º–µ–ª–æ —Å–ø—Ä–∞—à–∏–≤–∞–π üòä –¢–∞–∫–∂–µ —Ç—ã –º–æ–∂–µ—à—å –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å –º–µ—Å—Ç–∞, –≥–¥–µ –µ—Å—Ç—å –∂–µ–ª—Ç—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ —ç—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–µ (–æ–¥–Ω–∞–∫–æ, —ç—Ç–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ).\n",
    "\n",
    "–ü—Ä–µ–¥–ª–∞–≥–∞—é —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–¥ –ø—Ä–æ–µ–∫—Ç–æ–º –≤ –¥–∏–∞–ª–æ–≥–µ: –µ—Å–ª–∏ —Ç—ã —Ä–µ—à–∏—à—å —á—Ç–æ-—Ç–æ –ø–æ–º–µ–Ω—è—Ç—å –ø–æ –º–æ–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º ‚Äî –ø–∏—à–∏ –æ–± —ç—Ç–æ–º (–≤—ã–±–µ—Ä–∏ –¥–ª—è —Å–≤–æ–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ü–≤–µ—Ç - —Ç–∞–∫ –º–Ω–µ –±—É–¥–µ—Ç –ª–µ–≥—á–µ —É–≤–∏–¥–µ—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è). –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –Ω–µ –ø–µ—Ä–µ–º–µ—â–∞–π, –Ω–µ –∏–∑–º–µ–Ω—è–π –∏ –Ω–µ —É–¥–∞–ª—è–π –º–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏. –í—Å–µ —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Å–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —Ç–≤–æ–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–µ–µ.\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>‚ÑπÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞: </b> –ü—Ä–∏–º–µ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "<b> –í—Å—Ç—É–ø–ª–µ–Ω–∏–µ:</b> –†–∞–±–æ—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –Ω–∞ —Ö–æ—Ä–æ—à–µ–º —É—Ä–æ–≤–Ω–µ! –í–≤–∏–¥—É –Ω–µ–æ–±—ä—è—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–º–∞—Ç–∏–∫–∏ –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–æ–≤, —Ö–æ—á—É –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ª–µ–∑–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –æ—Ç —Ä–µ–≤—å—é–≤–µ—Ä–æ–≤ (–µ—Å–ª–∏ –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç):\n",
    "    \n",
    "–û—Å–Ω–æ–≤—ã Natural Language Processing –¥–ª—è —Ç–µ–∫—Å—Ç–∞  ‚Äî https://habr.com/ru/company/Voximplant/blog/446738/\n",
    "    \n",
    "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è —Å—Ç–∞—Ç—å—è –≤ –æ–±—â–µ–º –ø–æ NLP ‚Äî https://habr.com/ru/company/oleg-bunin/blog/352614/\n",
    "    \n",
    "BERT, ELMO –∏ –ö–æ –≤ –∫–∞—Ä—Ç–∏–Ω–∫–∞—Ö (–∫–∞–∫ –≤ NLP –ø—Ä–∏—à–ª–æ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ) ‚Äî https://habr.com/ru/post/487358/\n",
    "    \n",
    "–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, BERT —è–≤–ª—è–µ—Ç—Å—è –¥–æ–≤–æ–ª—å–Ω–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ–π –º–æ–¥–µ–ª—å—é. –í–æ—Ç –µ—â–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏:\n",
    "    \n",
    "https://habr.com/ru/post/436878/\n",
    "    \n",
    "http://jalammar.github.io/illustrated-bert/; https://huggingface.co/docs/transformers/model_doc/bert\n",
    "    \n",
    "–ò –¥—Ä—É–≥–∏–µ —Å—Å—ã–ª–∫–∏: \n",
    "    \n",
    "http://jalammar.github.io/illustrated-word2vec/\n",
    "    \n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "    \n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "    \n",
    "https://colab.research.google.com/drive/1SBha91u6142pW5mSiQOOjJWCCSAFj4HI?usp=sharing    \n",
    " \n",
    "</font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ—Ç –±—ã—Ç—å —Ç–µ–±–µ –±—É–¥–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –ø—Ä–∏–º–µ—Ä–æ–º BERT —Å GPU:\n",
    "\n",
    "```python\n",
    "%%time\n",
    "from tqdm import notebook\n",
    "batch_size = 2 # –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ –≤–æ–∑—å–º–µ–º —Ç–∞–∫–æ–π –±–∞—Ç—á, –≥–¥–µ –±—É–¥–µ—Ç –≤—Å–µ–≥–æ –¥–≤–µ —Å—Ç—Ä–æ–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "embeddings = [] \n",
    "for i in notebook.tqdm(range(input_ids.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(input_ids[batch_size*i:batch_size*(i+1)]).cuda() # –∑–∞–∫–∏–¥—ã–≤–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –Ω–∞ GPU\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.cuda()\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy()) # –ø–µ—Ä–µ–≤–æ–¥ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –ø—Ä–æ—Ü, —á—Ç–æ–±—ã –≤ –Ω—É–º–ø–∞–π –∫–∏–Ω—É—Ç—å\n",
    "        del batch\n",
    "        del attention_mask_batch\n",
    "        del batch_embeddings\n",
    "        \n",
    "features = np.concatenate(embeddings) \n",
    "```\n",
    "–ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –Ω–∞–ª–∏—á–∏–µ GPU.\\\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä, —Ç–∞–∫: ```device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")```\\\n",
    "–¢–æ–≥–¥–∞ –≤–º–µ—Å—Ç–æ .cuda() –Ω—É–∂–Ω–æ –ø–∏—Å–∞—Ç—å .to(device)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>‚ÑπÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞: </b> <br>\n",
    "\n",
    "**–ü—Ä–∏–≤–µ—Ç, –ê—Ä—Å–µ–Ω!** <br>\n",
    "    \n",
    "–û–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ –∑–∞ —Ç–∞–∫–æ–µ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –∫–æ–µ-—á—Ç–æ —è —É–∂–µ –æ—Ç–∫—Ä—ã–ª –ø–æ—á–∏—Ç–∞—Ç—å! <br>\n",
    "\n",
    "–ï—â—ë –≤ —Ç—Ä–µ–Ω–∞–∂—ë—Ä–µ —è –æ–±—Ä–∞—Ç–∏–ª –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–µ—â–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ progress bar, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–µ–ª –±—ã –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å. –í —Ö–æ–¥–µ —Ä–∞–±–æ—Ç—ã (—ç—Ç–æ –±—ã–ª —Ç—Ä–µ—Ç–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ—Å–ª–µ 2-—Ö —Ç—É–ø–∏–∫–æ–≤ —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º–∏ –æ—Ç–∫–∞—Ç–∞–º–∏ –≤–µ—Ä—Å–∏–∏) —è –æ–±—Ä–∞—Ç–∏–ª –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å —Ç–∏–ø–∏—á–Ω–æ —Ö–æ—Ä–æ—à–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞–º–∏ (RandomForest –∏ LGBM) –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–∞—é—Ç –Ω–µ —Ç–∞–∫–∏–µ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∫–∞–∫ –≤ –ø—Ä–æ—à–ª—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö –∏–∑ —á–µ–≥–æ —è —Å–¥–µ–ª–∞–ª –≤—ã–≤–æ–¥, —á—Ç–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ç–∏–ø–∞ `Bert`, –Ω–æ —Ç–∞–∫ –∏ –Ω–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω—è–ª –ø–æ–ø—ã—Ç–æ–∫ –≤ —ç—Ç–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ 3-–º –ø—Ä–∏—á–∏–Ω–∞–º: <br>\n",
    "    - –º–æ–∏ –¥–æ–º–∞—à–Ω–∏–µ –¥–µ–ª–∞, –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ –¥–∞—é—Ç –º–Ω–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å–≤–æ–±–æ–¥—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è –≤ —ç—Ç—É –æ–±–ª–∞—Å—Ç—å –Ω–∞ –¥–æ–ª–∂–Ω–æ–º —É—Ä–æ–≤–Ω–µ; <br>\n",
    "    - –º–æ–π —Å–∫–ª–∞–¥ —É–º–∞ –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–Ω–µ —Å—Ö–æ–¥—É –≤–Ω–∏–∫–∞—Ç—å –≤–æ –≤—Å–µ –¥–µ—Ç–∞–ª–∏ –∏–∑-–∑–∞ —á–µ–≥–æ —è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞—é –∏ –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞—é –ø–æ 2-3 —Ä–∞–∑–∞, —Ç.–∫. –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —É –º–µ–Ω—è –ø–æ—è–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞–≤—ã–∫, –Ω–æ –Ω–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –î–ª—è –º–µ–Ω—è —Ç–æ–ª—å–∫–æ –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –æ—Å–Ω–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–∫—Ä–µ–ø–∏—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, —Ç.–µ. –ø–æ–∫–∞ —è –Ω–µ —Ä–∞–∑–±–µ—Ä—É—Å—å –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å—É—Ç–∏ –º–µ—Ç–æ–¥–æ–≤, –ø–æ–∫–∞ –Ω–µ —É–≤–∏–∂—É –∫—Ä–∞—Å–æ—Ç—É —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–µ–º-—Ç–æ –ø–æ–¥—Ö–æ–¥–æ–≤, –¥–ª—è –º–µ–Ω—è —ç—Ç–æ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –∫–æ–º–±–∏–Ω–∞—Ü–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π, –∫–æ—Ç–æ—Ä—É—é —è –≤ –ª—É—á—à–µ–º —Å–ª—É—á–∞–µ —Å–º–æ–≥—É –ø—Ä–æ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å. –ü–æ—ç—Ç–æ–º—É, –µ—Å–ª–∏ –≤ –±—É–¥—É—â–µ–º –¥–æ–π–¥—ë—Ç –¥–æ —ç—Ç–æ–≥–æ, —Ç–æ —è –±—É–¥—É –µ—â—ë –Ω–µ –æ–¥–∏–Ω —Ä–∞–∑ –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞—Ç—å –∏ –ø—Ä–æ–±–æ–≤–∞—Ç—å –≤—Å—ë, —á—Ç–æ –±—ã–ª–æ –≤ —Ç—Ä–µ–Ω–∞–∂—ë—Ä–µ –∏ –≤—Å—ë —á—Ç–æ —Ç—ã –ø—Ä–∏—Å–ª–∞–ª; <br>\n",
    "    - –†–∞–±–æ—Ç–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –ø–æ–∫–∞–∑–∞–ª–∞—Å—å –º–Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–π, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π. –ü–æ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–≤–æ–µ–º—É –æ–ø—ã—Ç—É —Ä–∞–±–æ—Ç—ã —è - –∏–Ω–∂–µ–Ω–µ—Ä-—Ç–µ—Ö–Ω–æ–ª–æ–≥ —Å 10-—Ç–∏ –ª–µ—Ç–Ω–∏–º —Å—Ç–∞–∂–µ–º –∏ –≤—Å—ë, —á—Ç–æ –±—ã–ª–æ –Ω–∞ –∫—É—Ä—Å–µ –¥–æ —ç—Ç–æ–≥–æ —è —É–∂–µ –º—ã—Å–ª–µ–Ω–Ω–æ \"–ø—Ä–∏–∫—Ä—É—Ç–∏–ª\" –∫ —Å–≤–æ–µ–π —Ä–∞–±–æ—Ç–µ –∏ —É –º–µ–Ω—è —É–∂–µ, –∫–∞–∫ –≤ –º—É–ª—å—Ç–∏–∫–µ –ø—Ä–æ –°–∫—Ä—É–¥–∂–∞ –ú–∞–∫–î–∞–∫–∞, –≤ –≥–ª–∞–∑–∞—Ö –∫—Ä—É—Ç—è—Ç—Å—è —Ü–∏—Ñ—Ä—ã —Å —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–Ω—ã–º–∏ –¥–µ–Ω—å–≥–∞–º–∏ –æ—Ç –≤–Ω–µ–¥—Ä–µ–Ω–∏—è ML –≤ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–¥–µ–ª–∞—Ö. –ù–æ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥c—Ç–≤–µ –Ω–µ—Ç —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –∏ –≤ —ç—Ç–æ–º —Å–º—ã—Å–ª–µ —Ç–µ–∫—É—â–∏–π –º–æ–¥—É–ª—å —è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞, –ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ –ø–æ–∫–∞. –û–¥–Ω–∞–∫–æ, –∫—Ç–æ –∑–Ω–∞–µ—Ç, –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –±—É–¥—É—â–µ–º —è —É–π–¥—É —Å –≥–æ–ª–æ–≤–æ–π –≤ IT –∏ DS, –∏ —Ç–æ–≥–¥–∞, –≤–æ–∑–º–æ–∂–Ω–æ, —è –∑–∞–π–º—É—Å—å –≤–æ–ø—Ä–æ—Å–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –ø–æ–ª–Ω–æ–π. <br>\n",
    "    \n",
    "–í –æ–±—â–µ–º —Å–ø–∞—Å–∏–±–æ –µ—â—ë —Ä–∞–∑ –∑–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏. –í–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —ç—Ç–æ–π —á–∞—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã —É –º–µ–Ω—è –Ω–µ—Ç. –ü–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º –∏ –º–æ–¥–µ–ª—è–º –æ–Ω–∏, –≤–æ–∑–º–æ–∂–Ω–æ, –ø–æ—è–≤—è—Ç—Å—è –ø–æ–∑–∂–µ, –Ω–æ —Å–ø–µ—Ä–≤–∞ —è –≤—Å—ë —ç—Ç–æ –ø–µ—Ä–µ—á–∏—Ç–∞—é –∏ –ø–æ–ø—Ä–æ–±—É—é —Å–∞–º. <br>\n",
    "    \n",
    "–î–æ –Ω–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞ (v2):</b> –ú–∏—Ö–∞–∏–ª, —Å–ø–∞—Å–∏–±–æ –∑–∞ —Ç–∞–∫–æ–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π! –ú–Ω–µ —Ç–æ–∂–µ –µ—Å—Ç—å —á–µ–º –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –∏ –∑–∞–æ–¥–Ω–æ –æ—Ç–≤–µ—á—É –Ω–∞ —Ç–≤–æ–∏ –≤–æ–ø—Ä–æ—Å—ã =)\n",
    "    \n",
    "–ü–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é progress bar –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∑–¥–µ—Å—å: https://habr.com/ru/post/483400/\n",
    "    \n",
    "–ú–æ–¥–µ–ª–∏ –¥–µ—Ä–µ–≤—å–µ–≤ (RandomForest, DecisionTree) –≤ –∑–∞–¥–∞—á–∞—Ö NLP –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ. LGBM, CatBoost –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–ø–ª–æ—Ö–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. LogisticRegression –±—ã—Å—Ç—Ä–æ –æ–±—É—á–∞–µ—Ç—Å—è –∏ –ø—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ.\n",
    "    \n",
    "–ß—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –¥–µ–ª–∞—é—Ç —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –≤–µ–∫—Ç–æ—Ä–∞ (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏) –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å, —Ç—É –∂–µ LogisticRegression, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –≥–æ—Ä–∞–∑–¥–æ –≤—ã—à–µ (> 0.9).\n",
    "    \n",
    "–ü–µ—Ä–µ—á–∏—Ç—ã–≤–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫—É —Ä–∞–∑ —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –º–Ω–æ–≥–æ. –í–æ–æ–±—â–µ —è –¥–ª—è —Å–µ–±—è –≤—Å–µ–≥–¥–∞ –¥–µ–ª–∞—é –∫–æ–Ω—Å–ø–µ–∫—Ç, —Ç–∞–∫ –ª–µ–≥—á–µ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –∫–æ–≥–¥–∞ –≤—Ä–µ–º–µ–Ω–∏ –º–∞–ª–æ, –Ω–µ –≤—Å–µ–≥–¥–∞ —É–¥–∞–µ—Ç—Å—è –¥–æ–±—Ä–∞—Ç—å—Å—è –¥–æ —Å—É—Ç–∏, –ø–æ—ç—Ç–æ–º—É –≤ –∫–∞–∫–∏—Ö-—Ç–æ –≤–µ—â–∞—Ö —Å—Ç–æ–∏—Ç –≤—Å–µ –ø—Ä–∏–Ω—è—Ç—å –∫–∞–∫ –µ—Å—Ç—å, –∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–¥–µ—Ç –ø–æ–∑–∂–µ =)\n",
    "    \n",
    "–Ø –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ —Ç–µ–±—è –ø–æ–Ω–∏–º–∞—é, —Ç.–∫. —Å–∞–º –∏–Ω–∂–µ–Ω–µ—Ä –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é, –≤ DS –ø—Ä–∏—à–µ–ª —Ç–æ–ª—å–∫–æ –≤ 2020 –≥–æ–¥—É, –¥–æ —ç—Ç–æ–≥–æ –æ–∫–æ–ª–æ 10 –ª–µ—Ç –∑–∞–Ω–∏–º–∞–ª—Å—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Ä–∞—Å—á–µ—Ç–∞–º–∏. –¢–∞–∫–∂–µ –≤—Å–µ–≥–¥–∞ —Å—Ç–∞—Ä–∞–ª—Å—è –≤–Ω–∏–∫–Ω—É—Ç—å –≤ —Å—É—Ç—å –¥–µ–ª–∞, –ø–æ–¥–æ–ª–≥—É —Ä–∞–∑–±–∏—Ä–∞–ª—Å—è, —ç—Ç–æ –∫–æ–Ω–µ—á–Ω–æ –ø—Ä–∏–Ω–æ—Å–∏–ª–æ –ø–æ–ª—å–∑—É. –ù–æ –∫–æ–≥–¥–∞ –æ–∫–æ–Ω—á–∏–ª —Å–≤–æ–π –ø–µ—Ä–≤—ã–π –∫—É—Ä—Å –ø–æ DS –ø–æ–Ω—è–ª —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–∞—á–∞–ª–æ –ø—Ä–∏—Ö–æ–¥–∏—Ç—å, —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ —Å—Ç–∞–ª —Ä–µ—à–∞—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏.\n",
    "    \n",
    "–†–∞–±–æ—Ç—É –∏–Ω–∂–µ–Ω–µ—Ä–æ–º —è –∑–∞–≤–µ—Ä—à–∏–ª, —Å–µ–π—á–∞—Å —Ä–∞–±–æ—Ç–∞—é –∏ —Ä–∞–∑–≤–∏–≤–∞—é—Å—å –≤ –æ–±–ª–∞—Å—Ç–∏ DS.\n",
    "    \n",
    "–ñ–µ–ª–∞—é —Ç–µ–±–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–∫–æ–Ω—á–∏—Ç—å –∫—É—Ä—Å, –Ω–∞–π—Ç–∏ —Å–≤–æ–µ –º–µ—Å—Ç–æ –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ Data Science, –∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Å–∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞ —Ä–∞–±–æ—Ç–µ, –µ—Å–ª–∏ —Ç–∞–º –ø–ª–∞–Ω–∏—Ä—É–µ—à—å –ø—Ä–∏–º–µ–Ω—è—Ç—å ML =)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞-–¥–∞–Ω–Ω—ã—Ö\" data-toc-modified-id=\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞-–¥–∞–Ω–Ω—ã—Ö-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö</a></span><ul class=\"toc-item\"><li><span><a href=\"#–ò–º–ø–æ—Ä—Ç-–±–∏–±–ª–∏–æ—Ç–µ–∫\" data-toc-modified-id=\"–ò–º–ø–æ—Ä—Ç-–±–∏–±–ª–∏–æ—Ç–µ–∫-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>–ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫</a></span></li><li><span><a href=\"#–ó–∞–≥—Ä—É–∑–∫–∞-–¥–∞–Ω–Ω—ã—Ö\" data-toc-modified-id=\"–ó–∞–≥—Ä—É–∑–∫–∞-–¥–∞–Ω–Ω—ã—Ö-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö</a></span></li><li><span><a href=\"#–ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ-—Å-–¥–∞–Ω–Ω—ã–º–∏\" data-toc-modified-id=\"–ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ-—Å-–¥–∞–Ω–Ω—ã–º–∏-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>–ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å –¥–∞–Ω–Ω—ã–º–∏</a></span></li><li><span><a href=\"#–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\" data-toc-modified-id=\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞</a></span></li></ul></li><li><span><a href=\"#–û–±—É—á–µ–Ω–∏–µ\" data-toc-modified-id=\"–û–±—É—á–µ–Ω–∏–µ-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>–û–±—É—á–µ–Ω–∏–µ</a></span><ul class=\"toc-item\"><li><span><a href=\"#LogisticRegression\" data-toc-modified-id=\"LogisticRegression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>LogisticRegression</a></span></li><li><span><a href=\"#RandomForestClassifier\" data-toc-modified-id=\"RandomForestClassifier-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>RandomForestClassifier</a></span></li><li><span><a href=\"#LGBMClassifier\" data-toc-modified-id=\"LGBMClassifier-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>LGBMClassifier</a></span></li><li><span><a href=\"#LogisticRegression---CountVectorizer\" data-toc-modified-id=\"LogisticRegression---CountVectorizer-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>LogisticRegression - CountVectorizer</a></span></li><li><span><a href=\"#–í—ã–≤–æ–¥\" data-toc-modified-id=\"–í—ã–≤–æ–¥-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>–í—ã–≤–æ–¥</a></span></li><li><span><a href=\"#–ü—Ä–æ–≤–µ—Ä–∫–∞-–º–æ–¥–µ–ª–µ–π-–Ω–∞-—Ç–µ—Å—Ç–æ–≤–æ–π-–≤—ã–±–æ—Ä–∫–µ\" data-toc-modified-id=\"–ü—Ä–æ–≤–µ—Ä–∫–∞-–º–æ–¥–µ–ª–µ–π-–Ω–∞-—Ç–µ—Å—Ç–æ–≤–æ–π-–≤—ã–±–æ—Ä–∫–µ-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ</a></span></li></ul></li><li><span><a href=\"#–í—ã–≤–æ–¥—ã\" data-toc-modified-id=\"–í—ã–≤–æ–¥—ã-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>–í—ã–≤–æ–¥—ã</a></span></li><li><span><a href=\"#–ß–µ–∫-–ª–∏—Å—Ç-–ø—Ä–æ–≤–µ—Ä–∫–∏\" data-toc-modified-id=\"–ß–µ–∫-–ª–∏—Å—Ç-–ø—Ä–æ–≤–µ—Ä–∫–∏-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>–ß–µ–∫-–ª–∏—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–µ–∫—Ç –¥–ª—è ¬´–í–∏–∫–∏—à–æ–ø¬ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω ¬´–í–∏–∫–∏—à–æ–ø¬ª –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å. –¢–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤, –∫–∞–∫ –≤ –≤–∏–∫–∏-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö. –¢–æ –µ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö. –ú–∞–≥–∞–∑–∏–Ω—É –Ω—É–∂–µ–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∏—Ö –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é. \n",
    "\n",
    "–û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ. –í –≤–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π –æ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–æ–∫.\n",
    "\n",
    "–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –º–æ–¥–µ–ª—å —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ *F1* –Ω–µ –º–µ–Ω—å—à–µ 0.75. \n",
    "\n",
    "**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞**\n",
    "\n",
    "1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.\n",
    "2. –û–±—É—á–∏—Ç–µ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏. \n",
    "3. –°–¥–µ–ª–∞–π—Ç–µ –≤—ã–≤–æ–¥—ã.\n",
    "\n",
    "–î–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å *BERT* –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å.\n",
    "\n",
    "**–û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**\n",
    "\n",
    "–î–∞–Ω–Ω—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ñ–∞–π–ª–µ `toxic_comments.csv`. –°—Ç–æ–ª–±–µ—Ü *text* –≤ –Ω—ë–º —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è, –∞ *toxic* ‚Äî —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –•–æ—Ä–æ—à–æ, —á—Ç–æ –¥–æ–±–∞–≤–∏–ª –Ω–µ–±–æ–ª—å—à–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫.\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π IPython.\n",
    "from IPython.display import display_html\n",
    "from IPython.display import display, HTML, Math, Latex\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∞ –∏ CV\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer \n",
    "from sklearn.metrics import make_scorer,accuracy_score,f1_score,precision_score,recall_score\n",
    "from sklearn.metrics import precision_recall_curve,roc_auc_score,roc_curve\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–µ–π.\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º–æ–≥–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "from tqdm import notebook\n",
    "\n",
    "\n",
    "nltk.download('popular')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "pd.set_option('display.max_columns', None)         # –°–Ω–∏–º–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
    "pd.options.mode.chained_assignment = None          # –°–∫—Ä—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –º–∞—Å—à–∞—Ç–±–∏—Ä–æ–≤–∞–Ω–∏–∏\n",
    "pd.options.display.max_rows = None                 # –°–±—Ä–æ—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–≤–æ–¥–∏–º—ã—Ö —Ä—è–¥–æ–≤\n",
    "pd.set_option('display.max_colwidth', None)        # –°–±—Ä–æ—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∑–∞–ø–∏—Å–∏\n",
    "warnings.filterwarnings('ignore')                  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    toxic_comments = pd.read_csv('/datasets/toxic_comments.csv',sep=',')\n",
    "except:\n",
    "    toxic_comments = pd.read_csv('toxic_comments.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å –¥–∞–Ω–Ω—ã–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_info (df):\n",
    "    '''\n",
    "    –°–≤–æ–¥–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º\n",
    "    '''\n",
    "    display(df.head(10))\n",
    "    print('–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:')\n",
    "    display(df.info())\n",
    "    print('–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:')\n",
    "    display(df.describe())\n",
    "    print('–î—É–±–ª–∏–∫–∞—Ç—ã:')\n",
    "    display(df.duplicated().sum())\n",
    "    display(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the tools well. ¬†¬∑ talk \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"\\n\\nCongratulations from me as well, use the tools well. ¬†¬∑ talk \"   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.   \n",
       "8                                                                                                                                                            Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              alignment on this subject and which are contrary to those of DuLithgow   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      1  \n",
       "7      0  \n",
       "8      0  \n",
       "9      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.101679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic\n",
       "count  159571.000000\n",
       "mean        0.101679\n",
       "std         0.302226\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î—É–±–ª–∏–∫–∞—Ç—ã:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, toxic]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_info(toxic_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***–í—ã–≤–æ–¥—ã:*** <br>\n",
    "\n",
    "- *–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ 159571 —Å—Ç—Ä–æ–∫–∞*\n",
    "- *–ü—Ä–æ–ø—É—Å–∫–∏ –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç*\n",
    "- *–î—É–±–ª–∏–∫–∞—Ç—ã –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç*\n",
    "- *–ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–∏–º–≤–æ–ª—ã –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, —Ç.–µ. —Ç–µ–∫—Å—Ç –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ –æ—á–∏—Å—Ç–∫–µ*\n",
    "- *–ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ—Ä–∞–∑—ã –Ω–∞–ø–∏—Å–∞–Ω—ã CAPSLOCK'–æ–º*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –ó–¥–æ—Ä–æ–≤–æ, –ø—Ä–æ–≤–µ–ª –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö. \n",
    "    \n",
    "–í–∏–¥–Ω–æ, —á—Ç–æ –µ—Å—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤, —ç—Ç–æ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å –≤ –º–æ–¥–µ–ª—è—Ö.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearing_text(df,column_features):\n",
    "    \n",
    "    '''\n",
    "    –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–æ–¥–∏—Ç –æ—á–∏—Å—Ç–∫—É –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ–∑–¥–∞–µ—Ç –∫–æ—Ä–ø—É—Å\n",
    "    '''\n",
    "    \n",
    "    text_corp = df[column_features]\n",
    "    text_corp = text_corp.apply(lambda x: \" \".join((re.sub(r'[^a-zA-Z ]', ' ', x)).split()))\n",
    "    \n",
    "    return text_corp\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    \n",
    "    '''\n",
    "    –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–æ–¥–∏—Ç –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞\n",
    "    '''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_lem = corpus.apply(lambda x: [lemmatizer.lemmatize(w,'n') for w in nltk.word_tokenize(x)])\n",
    "    lemmatized = word_lem.apply(lambda text:' '.join(text))\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "\n",
    "def lower_case(text):\n",
    "    \n",
    "    '''\n",
    "    –§—É–Ω–∫—Ü–∏—è –ø—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    '''\n",
    "    \n",
    "    text_lower = text.map(lambda x: x if type(x)!=str else x.lower())\n",
    "    \n",
    "    return text_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –≤–µ—Ä–Ω–æ, –∫–æ–¥ –æ—Ñ–æ—Ä–º–ª–µ–Ω –∞–∫–∫—É—Ä–∞—Ç–Ω–æ =)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –ù–∞ –±—É–¥—É—â–µ–µ, –µ—Å–ª–∏ –±—É–¥–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –±–æ–ª–µ–µ —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤, –º–æ–∂–µ—à—å –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ —Å–ø–æ—Å–æ–±—ã –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏. \n",
    "    \n",
    "1. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Spacy. –ù–æ –µ—Å–ª–∏ –¥–µ–ª–∞—Ç—å –Ω–∞ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 0,5-1 —á–∞—Å, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–∞.\n",
    "https://spacy.io/ –∏–ª–∏ https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/\n",
    "\n",
    "2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å POS-—Ç–µ–≥–∏, —ç—Ç–æ —Å–¥–µ–ª–∞–µ—Ç –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é —Ç–æ—á–Ω–µ–µ:\n",
    "https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/\n",
    "    \n",
    "–≠—Ç–æ –∑–∞–Ω–∏–º–∞–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏, –ø–æ—ç—Ç–æ–º—É –º–æ–∂–µ—à—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–µ. –¢–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–µ—Ç–æ–¥ pos_tag_sents(), –∫–æ–≥–¥–∞ –ø–µ—Ä–µ–±–æ—Ä –∏–¥–µ—Ç –Ω–µ –ø–æ —Å–ª–æ–≤–∞–º, –∞ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º ‚Äî —ç—Ç–æ –Ω–µ–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = clearing_text(toxic_comments,'text')\n",
    "corpus = lemmatize(corpus)\n",
    "corpus = lower_case(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                                                                                                                                                                           explanation why the edits made under my username hardcore metallica fan were reverted they weren t vandalism just closure on some gas after i voted at new york dolls fac and please don t remove the template from the talk page since i m retired now\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d aww he match this background colour i m seemingly stuck with thanks talk january utc\n",
       "2                                                                                                                                                                                                                                                                                                                                                                             hey man i m really not trying to edit war it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info\n",
       "3    more i can t make any real suggestion on improvement i wondered if the section statistic should be later on or a subsection of type of accident i think the reference may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no one else doe first if you have any preference for formatting style on reference or want to do it yourself please let me know there appears to be a backlog on article for review so i guess there may be a delay until a reviewer turn up it s listed in the relevant form eg wikipedia good article nomination transport\n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   you sir are my hero any chance you remember what page that s on\n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             congratulations from me a well use the tool well talk\n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      cocksucker before you piss around on my work\n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    your vandalism to the matt shirvington article ha been reverted please don t do it again or you will be banned\n",
       "8                                                                                                                                                         sorry if the word nonsense wa offensive to you anyway i m not intending to write anything in the article wow they would jump on me for vandalism i m merely requesting that it be more encyclopedic so one can use it for school a a reference i have been to the selective breeding page but it s almost a stub it point to animal breeding which is a short messy article that give you no info there must be someone around with expertise in eugenics\n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            alignment on this subject and which are contrary to those of dulithgow\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***–í—ã–≤–æ–¥:*** <br>\n",
    "\n",
    "- *–¢–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç—Ä–∏–µ–≤ –±—ã–ª –æ—á–∏—â–µ–Ω –æ–Ω –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è*\n",
    "- *–ë—ã–ª–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞*\n",
    "- *–í–µ—Å—å —Ç–µ–∫—Å—Ç –±—ã–ª –ø–µ—Ä–µ–≤–µ–¥—ë–Ω –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sample(text, target):\n",
    "    \n",
    "    '''\n",
    "    –§—É–Ω–∫—Ü–∏—è –¥–µ–ª–∏—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "    '''\n",
    "    \n",
    "    features_train, features_test, target_train, target_test = train_test_split(text, target, train_size=0.80,random_state=12345)\n",
    "    \n",
    "    return (features_train, features_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä stratify = target, —ç—Ç–æ –æ–±–µ—Å–ø–µ—á–∏—Ç –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–∞—Ö.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: (127656,) (127656,)\n",
      "–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: (31915,) (31915,)\n"
     ]
    }
   ],
   "source": [
    "toxic = toxic_comments['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = split_sample(corpus, toxic)\n",
    "\n",
    "print('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏:',features_train.shape, target_train.shape)\n",
    "print('–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏:',features_test.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(train_text, test_text, vectorizer):\n",
    "    \n",
    "    '''\n",
    "    –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–æ–¥–∏—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞\n",
    "    '''\n",
    "    \n",
    "    counter = vectorizer(stop_words=stopwords)\n",
    "    vectorized_train = counter.fit_transform(train_text)\n",
    "    vectoruzed_test = counter.transform(test_text)\n",
    "    \n",
    "    \n",
    "    return vectorized_train, vectoruzed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train,tf_idf_test = vectorization(features_train,features_test,TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã TF-IDF –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: (127656, 144110)\n",
      "–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã TF-IDF –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: (31915, 144110)\n"
     ]
    }
   ],
   "source": [
    "print('–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã TF-IDF –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏:',tf_idf_train.shape)\n",
    "print('–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã TF-IDF –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏:',tf_idf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***–í—ã–≤–æ–¥:*** <br>\n",
    "\n",
    "- *–∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ —Ä–∞–∑–¥–µ–ª—ë–Ω –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏*\n",
    "- *–±—ã–ª–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –∫–∞–∂–¥–æ–π –∏–∑ –≤—ã–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤—ã–±–æ—Ä–æ–∫*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –ú–æ–∂–µ—à—å —Ç–∞–∫–∂–µ –ø–æ–ø—Ä–æ–±–æ–≤—Ç—å Pipeline + GridSearch. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Pipeline –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç–∫–∏ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏.\n",
    "    \n",
    "https://medium.com/analytics-vidhya/ml-pipelines-using-scikit-learn-and-gridsearchcv-fe605a7f9e05\n",
    "    \n",
    "https://habr.com/ru/post/538458/\n",
    "    \n",
    "Pipeline + GridSearchCV: https://towardsdatascience.com/nlp-with-pipeline-gridsearch-5922266e82f4\n",
    "    \n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –≤ GridSearchCV –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω ‚Äî –Ω–æ —ç—Ç–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_params (model,params,features,target):\n",
    "    \n",
    "    '''\n",
    "    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ–±—É—á–∞–µ–º–æ–π –º–æ–¥–µ–ª–∏, –Ω–∞–±–æ—Ä –µ—ë –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∫–æ—Ç–æ—Ä—ã—Ö —Ü–µ–ª–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ f1 (—Å—Ä–µ–¥–Ω–µ–µ –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ –ø–æ–ª–Ω–æ—Ç—ã –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏) –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞\n",
    "    '''\n",
    "    \n",
    "    model_grid = GridSearchCV(model,params,cv=3, n_jobs=-1,scoring='f1', verbose=2)\n",
    "    model_grid.fit(features,target)\n",
    "    best_params = model_grid.best_params_\n",
    "    best_score = model_grid.best_score_\n",
    "    \n",
    "    return model_grid,best_params,best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_list = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –ó–¥–æ—Ä–æ–≤–æ, —á—Ç–æ –¥–µ–ª–∞–µ—à—å –ø–µ—Ä–µ–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö. –î–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –º–æ–∂–Ω–æ –ø–æ–≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ \"C\", –æ–Ω–∞ —Ö–æ—Ä–æ—à–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –º–µ—Ç—Ä–∏–∫—É.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lr = {\"max_iter\":range(63,67,1)}\n",
    "model_lr = LogisticRegression(random_state = 12345,class_weight='balanced',n_jobs = -1, max_iter=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color='green'>f1 LogisticRegression: 0.747</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 453 ms\n",
      "Wall time: 27.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_iter': 65}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_grid_lr,best_params_lr,best_score_lr = grid_search_params(model_lr,params_lr,\n",
    "                                                                         tf_idf_train,target_train)\n",
    "\n",
    "display(HTML(f\"<font color='green'>f1 LogisticRegression: {best_score_lr:.3f}</font>\"))\n",
    "best_params_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_list['LogisticRegression'] = [best_params_lr,best_score_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_forest = {'max_depth': range(398,401,1),'n_estimators': range(8,10,1)}\n",
    "model_forest = RandomForestClassifier(random_state = 12345,class_weight='balanced',n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color='blue'>f1 RandomForestClassifier: 0.628</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 5s\n",
      "Wall time: 6min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 399, 'n_estimators': 9}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_grid_forest,best_params_forest,best_score_forest = grid_search_params(model_forest,params_forest,\n",
    "                                                                         tf_idf_train,target_train)\n",
    "\n",
    "display(HTML(f\"<font color='blue'>f1 RandomForestClassifier: {best_score_forest:.3f}</font>\"))\n",
    "best_params_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_list['RandomForest'] = [best_params_forest,best_score_forest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgbm = {\n",
    "    'objective': ['binary'],\n",
    "    'max_depth': range(6,8,1),\n",
    "    'learning_rate':np.arange(0.021,0.023,0.001),\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'metric': ['f1'],\n",
    "    'verbosity': [-10],\n",
    "    'random_state': [12345],\n",
    "    'n_estimators': range(400,501,50),    \n",
    "    }\n",
    "\n",
    "model_lgbm = LGBMClassifier(random_state = 12345,class_weight='balanced',n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color='red'>f1 LGBMClassifier:0.714</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 44s\n",
      "Wall time: 16min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'learning_rate': 0.022000000000000002,\n",
       " 'max_depth': 7,\n",
       " 'metric': 'f1',\n",
       " 'n_estimators': 500,\n",
       " 'objective': 'binary',\n",
       " 'random_state': 12345,\n",
       " 'verbosity': -10}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_grid_lgbm,best_params_lgbm,best_score_lgbm = grid_search_params(model_lgbm,params_lgbm,\n",
    "                                                                   tf_idf_train,target_train)\n",
    "\n",
    "display(HTML(f\"<font color='red'>f1 LGBMClassifier:{best_score_lgbm:.3f}</font>\"))\n",
    "best_params_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_list['LGBMClassifier'] = [best_params_lgbm,best_score_lgbm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***–í—ã–≤–æ–¥:***<br>\n",
    "\n",
    "*–ù–∏ –æ–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –º–µ—Ç—Ä–∏–∫–∏ f1*<br>\n",
    "*–ü–æ–ø—Ä–æ–±—É–µ–º –≤ –ª—É—á—à—É—é –ø–æ –º–µ—Ç—Ä–∏–∫–µ `f1` –º–æ–¥–µ–ª—å LogisticRegression –ø–µ—Ä–µ–¥–∞—Ç—å –Ω–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–º `CountVectorizer` –ø—Ä–∏–∑–Ω–∞–∫–æ–≤*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: (127656, 144110)\n",
      "–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: (31915, 144110)\n"
     ]
    }
   ],
   "source": [
    "vectorized_train,vectorized_test = vectorization(features_train,features_test,CountVectorizer)\n",
    "\n",
    "print('–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏:',vectorized_train.shape)\n",
    "print('–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏:',vectorized_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression - CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lr_CV = {\"max_iter\":range(68,72,1)}\n",
    "model_lr_CV = LogisticRegression(random_state = 12345,class_weight='balanced',n_jobs = -1, max_iter=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color='green'>f1 LogisticRegression: 0.752</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 453 ms\n",
      "Wall time: 21.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_iter': 70}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_grid_lr_CV,best_params_lr_CV,best_score_lr_CV = grid_search_params(model_lr_CV,\n",
    "                            params_lr_CV,vectorized_train,target_train)\n",
    "\n",
    "display(HTML(f\"<font color='green'>f1 LogisticRegression: {best_score_lr_CV:.3f}</font>\"))\n",
    "best_params_lr_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_list['LogisticRegression - CountVectorizer'] = [best_params_lr_CV,best_score_lr_CV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <th>LogisticRegression - CountVectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'max_iter': 65}</td>\n",
       "      <td>{'max_depth': 399, 'n_estimators': 9}</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'learning_rate': 0.022000000000000002, 'max_depth': 7, 'metric': 'f1', 'n_estimators': 500, 'objective': 'binary', 'random_state': 12345, 'verbosity': -10}</td>\n",
       "      <td>{'max_iter': 70}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.746793</td>\n",
       "      <td>0.627933</td>\n",
       "      <td>0.713566</td>\n",
       "      <td>0.752134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LogisticRegression                           RandomForest  \\\n",
       "params     {'max_iter': 65}  {'max_depth': 399, 'n_estimators': 9}   \n",
       "f1-score           0.746793                               0.627933   \n",
       "\n",
       "                                                                                                                                                                                 LGBMClassifier  \\\n",
       "params    {'boosting_type': 'gbdt', 'learning_rate': 0.022000000000000002, 'max_depth': 7, 'metric': 'f1', 'n_estimators': 500, 'objective': 'binary', 'random_state': 12345, 'verbosity': -10}   \n",
       "f1-score                                                                                                                                                                               0.713566   \n",
       "\n",
       "         LogisticRegression - CountVectorizer  \n",
       "params                       {'max_iter': 70}  \n",
       "f1-score                             0.752134  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_scores = pd.DataFrame(models_scores_list,index=['params', 'f1-score'])\n",
    "models_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***–° –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ–π –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ f1 > 0.75 —Å–º–æ–≥–ª–∞ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è –ª–∏—à—å –º–æ–¥–µ–ª—å LogisticRegression —Å –º–µ—Ç–æ–¥–æ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ CountVectorizer***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<font color='blue'>f1_score –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ LinearRegression —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π CountVectorizer:0.756</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#–û–±—É—á–∏–º –º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ —É–∂–µ –±–µ–∑ GridSearch –Ω–∞ —Ä–∞–Ω–µ–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö, –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö.\n",
    "\n",
    "model_log_best = LogisticRegression(random_state = 12345,class_weight='balanced',n_jobs = -1, max_iter=250)\n",
    "model_log_best.fit(vectorized_train,target_train)\n",
    "\n",
    "#–ø–æ–ª—É—á–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏, —á—Ç–æ–±—ã —Å—Ä–∞–≤–Ω–∏—Ç—å –∏–º–µ—é—â–∏–º—Å—è –≤–µ–∫—Ç–æ—Ä–æ–º —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞.\n",
    "predicted = model_log_best.predict(vectorized_test)\n",
    "\n",
    "#–í—ã—á–∏—Å–ª–∏–º f1 –º–µ—Ä—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.\n",
    "score_best = f1_score(target_test,predicted)\n",
    "display(HTML(f\"<font color='blue'>f1_score –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ LinearRegression —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π CountVectorizer:\\\n",
    "{score_best:.3f}</font>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>‚úîÔ∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –ü–æ–∑–¥—Ä–∞–≤–ª—è—é, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –ø–æ—Ä–æ–≥ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é F-–º–µ—Ä—ã –ø—Ä–æ–π–¥–µ–Ω!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***–°—Ä–µ–¥–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ª—É—á—à–µ–π –∏ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–µ–π —É—Å–ª–æ–≤–∏—é –∑–∞–¥–∞—á–∏ –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –ø–æ—Ä–æ–≥—É `f1`–º–µ—Ä—ã (—Å—Ä–µ–¥–Ω–µ–≥–æ –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–ª–Ω–æ—Ç—ã-`recall`–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏-`precision`) —Å—Ç–∞–ª–∞ –º–æ–¥–µ–ª—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –≤—ã–±–æ—Ä–∫–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–º CountVectorizer.*** <br>\n",
    "***–ü—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ `f1`= 0.756***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "<b> ‚úîÔ∏è üëç–ó–∞–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–≤—å—é–≤–µ—Ä–∞:</b> –ú–∏—Ö–∞–∏–ª, –±—ã–ª–æ –ø—Ä–∏—è—Ç–Ω–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ç–≤–æ—é —Ä–∞–±–æ—Ç—É, –æ–Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –Ω–∞ —Ö–æ—Ä–æ—à–µ–º —É—Ä–æ–≤–Ω–µ! \n",
    "   \n",
    "–ö–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω, –∑–¥–æ—Ä–æ–≤–æ, —á—Ç–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Å–¥–µ–ª–∞–ª –ø–µ—Ä–µ–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
    "    \n",
    "–û—Å–æ–±—ã—Ö –∑–∞–º–µ—á–∞–Ω–∏–π –ø–æ —Ä–∞–±–æ—Ç–µ –Ω–µ—Ç, —è –æ—Ç–º–µ—Ç–∏–ª —Ç–æ–ª—å–∫–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.\n",
    "    \n",
    "–û—Ç–ø—Ä–∞–≤–ª—è—é —Ä–∞–±–æ—Ç—É –¥–ª—è –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è —Å —Ö–æ–¥–æ–º –ø—Ä–æ–≤–µ—Ä–∫–∏, –∞ —Ç–∞–∫–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã ‚Äî —Å–º–µ–ª–æ –∑–∞–¥–∞–≤–∞–π –∏—Ö, —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å —Ç–µ–±–µ –ø–æ–º–æ—á—å üòä\n",
    "\n",
    "–ñ–¥—É —Ç–≤–æ–µ–≥–æ –æ—Ç–≤–µ—Ç–∞!\n",
    " </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–µ–∫-–ª–∏—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook –æ—Ç–∫—Ä—ã—Ç\n",
    "- [x]  –í–µ—Å—å –∫–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫\n",
    "- [x]  –Ø—á–µ–π–∫–∏ —Å –∫–æ–¥–æ–º —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –≤ –ø–æ—Ä—è–¥–∫–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "- [x]  –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã\n",
    "- [x]  –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã\n",
    "- [x]  –ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ *F1* –Ω–µ –º–µ–Ω—å—à–µ 0.75\n",
    "- [x]  –í—ã–≤–æ–¥—ã –Ω–∞–ø–∏—Å–∞–Ω—ã"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 95,
    "start_time": "2022-06-23T14:37:09.444Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
